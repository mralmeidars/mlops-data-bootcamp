{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/24 11:08:29 WARN Utils: Your hostname, allmey-linux resolves to a loopback address: 127.0.1.1; using 192.168.0.98 instead (on interface wlp2s0)\n",
      "23/05/24 11:08:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/24 11:08:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql.functions import col, stddev, unix_timestamp\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "s3EndPointUrl = \"http://localhost:9000\"\n",
    "s3AccessKeyAws = \"minio\"\n",
    "s3SecretKeyAws = \"minioadmin\"\n",
    "sourceBucket = \"mlopscamp\"\n",
    "\n",
    "sparkApp = SparkSession\\\n",
    "                .builder\\\n",
    "                .appName(\"homework-week-1\")\\\n",
    "                .config(\"spark.hadoop.fs.s3a.endpoint\", s3EndPointUrl)\\\n",
    "                .config(\"spark.hadoop.fs.s3a.access.key\", s3AccessKeyAws)\\\n",
    "                .config(\"spark.hadoop.fs.s3a.secret.key\", s3SecretKeyAws)\\\n",
    "                .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\\\n",
    "                .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\\\n",
    "                .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\\\n",
    "                .config(\"spark.hadoop.fs.s3a.fast.upload\", \"true\")\\\n",
    "                .config(\"spark.sql.execution.pyarrow.enabled\", \"true\")\\\n",
    "                .config(\"spark.sql.inMemoryColumnarStorage.compressed\", \"true\")\\\n",
    "                .config(\"spark.sql.inMemoryColumnarStorage.batchSize\", 10000)\\\n",
    "                .config(\"spark.sql.shuffle.partitions\", 100)\\\n",
    "                .config(\"spark.sql.debug.maxToStringFields\", 1000)\\\n",
    "                .getOrCreate()\n",
    "\n",
    "sparkApp.sparkContext.setLogLevel(\"OFF\")\n",
    "\n",
    "yellow_files_1 = f\"s3a://{sourceBucket}/yellow_tripdata_2022-01.parquet\"\n",
    "yellow_files_2 = f\"s3a://{sourceBucket}/yellow_tripdata_2022-02.parquet\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_yellow_1 = sparkApp.read\\\n",
    "                    .format(\"parquet\")\\\n",
    "                    .option(\"inferSchema\", True)\\\n",
    "                    .option(\"header\", True)\\\n",
    "                    .load(yellow_files_1)\n",
    "\n",
    "df_yellow_2 = sparkApp.read\\\n",
    "                      .format(\"parquet\")\\\n",
    "                      .option(\"inferSchema\", True)\\\n",
    "                      .option(\"header\", True)\\\n",
    "                      .load(yellow_files_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of Columns in the Dataset:  19\n"
     ]
    }
   ],
   "source": [
    "# Q1. Count columns\n",
    "# Read the data for January. How many columns are there ?\n",
    "#\n",
    "print('Total of Columns in the Dataset: ', len(df_yellow_1.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:===================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Deviation of the Trips:  [Row(stddev_samp(DurationInMinutes)=46.44530513776847)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Q2 - Computing Duration\n",
    "# What's the standard deviation of the trips duration in January ?\n",
    "#\n",
    "# Create new column to calculate the duration trip\n",
    "df_yellow_1 = df_yellow_1.withColumn('Duration', (unix_timestamp('tpep_dropoff_datetime') - unix_timestamp('tpep_pickup_datetime')))\n",
    "df_yellow_1 = df_yellow_1.withColumn('DurationInMinutes', col('Duration')/60)\n",
    "\n",
    "result = df_yellow_1.select(stddev('DurationInMinutes')).collect()\n",
    "\n",
    "print('Standard Deviation of the Trips: ', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of the records left after dropped:  98.27547930522405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Q3 - Dropping Outliers\n",
    "# What fraction of the records left after you dropped the outliers ?\n",
    "#\n",
    "raw_count = df_yellow_1.count()\n",
    "df_yellow_1 = df_yellow_1.filter((col('DurationInMinutes') >= 1) & (col('DurationInMinutes') <= 60))\n",
    "\n",
    "print('Fraction of the records left after dropped: ', (df_yellow_1.count()/raw_count)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow_1.selectExpr('cast(PULocationID as string) PULocationID')\n",
    "df_yellow_1.selectExpr('cast(DOLocationID as string) DOLocationID')\n",
    "df_yellow_1.selectExpr('cast(DurationInMinutes as double) DurationInMinutes')\n",
    "categorical_features = ['PULocationID', 'DOLocationID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionality of the matrix:  2\n"
     ]
    }
   ],
   "source": [
    "# Q4 - One-Hot Encoding\n",
    "# Let's apply one-hot encoding to the pickup and dropoff location IDs. We'll use only these two features for our model.\n",
    "#\n",
    "from sklearn.feature_extraction import DictVectorizer # Machine Learning\n",
    "\n",
    "# Turn the dataframe into a list of dictionaries\n",
    "dict_yellow_1 = df_yellow_1[categorical_features].toPandas().to_dict(orient='records')\n",
    "\n",
    "# Instantiate a dictionary vectorizer\n",
    "dv = DictVectorizer()\n",
    "\n",
    "# Fit a dictionary vectorizer - transform the data into a feature matrix\n",
    "x_train = dv.fit_transform(dict_yellow_1)\n",
    "\n",
    "print('Dimensionality of the matrix: ', x_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on train:  8.920327827581282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Q5 - Training Model\n",
    "# Now let's use the feature matrix from the previous step to train a model. \n",
    "# * Train a plain linear regression model with default parameters \n",
    "# * Calculate the RMSE of the model on the training data\n",
    "# What's the RMSE on train ?\n",
    "#\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=categorical_features, outputCol='features')\n",
    "x_train = assembler.transform(df_yellow_1)\n",
    "\n",
    "target='DurationInMinutes'\n",
    "lr = LinearRegression(featuresCol='features', labelCol=target)\n",
    "lr_model = lr.fit(x_train)\n",
    "\n",
    "trainSummary = lr_model.summary\n",
    "\n",
    "print('RMSE on train: ', trainSummary.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on train - February 2022:  9.573242692015372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Q6 - Evaluating the model \n",
    "# Now let's apply this model to the validation dataset (February 2022).\n",
    "# What's the RMSE on validation ?\n",
    "#\n",
    "# Create new column to calculate the time trip\n",
    "df_yellow_2 = df_yellow_2.withColumn('Duration', (unix_timestamp('tpep_dropoff_datetime') - unix_timestamp('tpep_pickup_datetime')))\n",
    "df_yellow_2 = df_yellow_2.withColumn('DurationInMinutes', col('Duration')/60)\n",
    "df_yellow_2 = df_yellow_2.filter((col('DurationInMinutes') >= 1) & (col('DurationInMinutes') <= 60))\n",
    "\n",
    "df_yellow_2.selectExpr('cast(PULocationID as string) PULocationID')\n",
    "df_yellow_2.selectExpr('cast(DOLocationID as string) DOLocationID')\n",
    "df_yellow_2.selectExpr('cast(DurationInMinutes as double) DurationInMinutes')\n",
    "\n",
    "assembler = VectorAssembler(inputCols=categorical_features, outputCol='features')\n",
    "\n",
    "x_train_2 = assembler.transform(df_yellow_2)\n",
    "lr_model_2 = lr.fit(x_train_2)\n",
    "\n",
    "trainSummary = lr_model_2.summary\n",
    "\n",
    "print('RMSE on train - February 2022: ', trainSummary.rootMeanSquaredError)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
